---
title: 卷积神经网络CNN入门
tags: [AI, Deep learning, CNN]
---

全连接神经网络最大问题是参数爆炸，容易影响计算速度和导致过拟合问题。而CNN则通过局部连接、权值共享等方法避免了这一难题。

<!-- more -->

### CNN 一般架构
![](http://xiaoluban.bj.bcebos.com/laphiler%2FCNN_startup%2Fcnn_struct.jpg?authorization=bce-auth-v1%2F94767b1b37b14a259abca0d493cefafa%2F2017-12-20T08%3A13%3A08Z%2F-1%2Fhost%2Ff5b2ebb781f1e557b33cf2299168e5377e053f3c2fd2fffa909ad73c8566b6f2)

### 输入层
CNN通过每层不同的网络结构，将上一层的输出转化为下一层输入。
### 卷积层（Convolutional Layer）
**卷积核**：卷积层的过滤器(filter)或者叫内核(kernel)既**卷积核**，可以将当前层神经网络上的一个子节点矩阵转化为下一层神经网络上的一个单位节点矩阵。
卷积核的尺寸(size)或者长和宽是用人工指定的，常用的卷积核尺寸有3x3或者5x5，而卷积核处理的矩阵深度就是当前层神经网络节点的深度，顾不需要人工指定。而卷积核的深度指的是处理得到的单位矩阵的深度。
> 卷积核的尺寸指的是输入节点矩阵的大小(例5x5)，卷积核的深度指的是输出单位矩阵的深度(例1x1xZ).

**前向传播算法**:
卷积核前向传播过程就是用左侧矩阵中的节点通过激活函数(比如ReLU)计算出右侧单位矩阵中节点的过程。
### 池化层（Pooling Layer）
在卷积层之间往往会加上一个池化层(pooling layer)，来缩小矩阵的尺寸，从而减少最后全连接层中的参数。池化层既可以加快计算速度，也有防止过拟合的作用。
池化层的计算：池化层的前向传播过程也是通过类似卷积核的结构完成的，但是卷积核中的计算不是节点的加权和，而是通过简单的最大值或者平均值运算，相应的称为最大池化层(max pooling)和平均池化层(average pooling)。实
践中，池化层的应用较少。

### 激活函数层（Activation Layer）
##### 激活函数的作用
首先激活函数并不是真的能激活什么，在神经网络中，激活函数的作用是能够给神经网络加入一些**非线性因素**，使得神经网络可以逼近基本上所有的函数，从而解决非常复杂的问题。例如在分类中，当数据集线性不可分时，就需要非线性激活函数来拟合出一条非线性函数，对样本数据进行分类。如下图：

![](http://xiaoluban.bj.bcebos.com/laphiler%2FCNN_startup%2Factivation_func.jpg?authorization=bce-auth-v1%2F94767b1b37b14a259abca0d493cefafa%2F2018-01-03T06%3A28%3A38Z%2F-1%2Fhost%2Fccb12ae2f7d66656791806ac974b63b53bdfa66cde593dab7e5c783239f7d33f)

##### 激活函数的性质
激活函数通常有如下一些性质：

- **非线性**： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=x），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。
- **可微性**： 当优化方法是基于梯度的时候，这个性质是必须的。
- **单调性**： 当激活函数是单调的时候，单层网络能够保证是凸函数。
- **f(x)≈x**： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。
- **输出值的范围**： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.

这些性质，也正是我们使用激活函数的原因。
##### 常见的激活函数
- sigmoid：控制在[0, 1]
- tanh：控制在[-1, 1]
- ReLU：控制在[0, 正无穷]

![](http://xiaoluban.bj.bcebos.com/laphiler%2FCNN_startup%2FReLU.png?authorization=bce-auth-v1%2F94767b1b37b14a259abca0d493cefafa%2F2018-01-03T06%3A25%3A36Z%2F-1%2Fhost%2F7e0fb79fce6a170582799f1ad34df6baa4f9f89fa45c34e906db2f8b99625b75)

数学公式：
$$f(x) = max(0,x)$$

当输入 x<0 时，输出为 0，当 x> 0 时，输出为 x。该激活函数使网络更快速地收敛。它不会饱和，即它可以对抗梯度消失问题，至少在正区域（x> 0 时）可以这样，因此神经元至少在一半区域中不会把所有零进行反向传播。由于使用了简单的阈值化（thresholding），ReLU 计算效率很高。但是 ReLU 神经元也存在一些缺点：
> 
1. 不以零为中心：和 Sigmoid 激活函数类似，ReLU 函数的输出不以零为中心。
>
2. 前向传导（forward）过程中，如果 x < 0，则神经元保持非激活状态，且在后向传导（backward）中「杀死」梯度。这样权重无法得到更新，网络无法学习。当 x = 0 时，该点的梯度未定义，但是这个问题在实现中得到了解决，通过采用左侧或右侧的梯度的方式。

为了解决 ReLU 激活函数中的梯度消失问题，当 x < 0 时，可以使用 Leaky ReLU——该函数试图修复 dead ReLU 问题。
ReLU是最常用的非线性激活函数，还有很多变种，这里不细说。

##### 比较
**sigmoid 缺点**：

- 两头过于平坦
- 输出值域不对称（非0均值）

**tanh 缺点**:

- 两头依旧过于平坦

**ReLU**:

- 收敛速度比 sigmoid/tanh 更快
- ReLU本质上是分段线性模型，前向计算非常简单，无需指数之类操作；
- ReLU的偏导也很简单，反向传播梯度，无需指数或者除法之类操作；
- ReLU不容易发生梯度发散问题，Tanh和sigmoid激活函数在两端的时候导数容易趋近于零，多级连乘后梯度更加约等于0；
- ReLU关闭了右边，从而会使得很多的隐层输出为0，即网络变得稀疏，起到了类似L1的正则化作用，可以在一定程度上缓解过拟合。
- 当然，ReLU也是有缺点的，比如左边全部关了很容易导致某些隐藏节点永无翻身之日，所以后来又出现pReLU、random ReLU等改进，而且ReLU会很容易改变数据的分布，因此ReLU后加**Batch Normalization**也是常用的改进的方法。

### 全连接层（Full Connected Layer）
经过多层卷积层和池化层处理，可以理解图像中的信息被抽象出信息含量更高的特征，待特征提取完后，CNN最后一般会通过1到2个全连接层给出分类结果。
**如何使网络接受任意的输入？**
有些时候需要把全连接层变成卷积层，这就是所谓的**卷积化**。这里需要证明卷积化的等价性。直观上理解，卷积跟全连接都是一个点乘的操作，区别在于卷积是作用在一个局部的区域，而全连接是对于整个输入而言，那么只要把
卷积作用的区域扩大为整个输入，那就变成全连接了。所以我们只需要把卷积核变成跟输入的一个map的大小一样就可以了，这样的话就相当于使得卷积跟全连接层的参数一样多。举个例子，比如 AlexNet，fc6 的输入是 256x6x6，>
那么这时候只需要把 fc6 变成是卷积核为6x6的卷积层就好了。

**与传统神经网络相比，CNN 参数和计算量更多还是更少了？**
参数变少了，因为都使用一套参数，而计算量却是变大了，因为卷积窗口要滑到不同的地方，进行计算、合并等操作。

### 优化
提高泛化能力（减少 overfit）

1. 增加神经网络层数。使用卷积层极大地减小了全连接层中的参数的数目，使学习的问题更容易
2. 使用更多强有力的规范化技术（尤其是 dropout 和 regularization）来减小过度拟合
3. 使用修正线性单元而不是 S 型神经元，来加速训练-依据经验，通常是3-5倍
4. 使用 GPU 来计算
5. 利用充分大的数据集，避免过拟合
6. 使用正确的代价函数，避免学习减速
7. 使用好的权重初始化，避免因为神经元饱和引起的学习减速